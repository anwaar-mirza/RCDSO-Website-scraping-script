# RCDSO-Website-scraping-script

## Website Overview

The **Royal College of Dental Surgeons of Ontario (RCDSO)** website ([rcdso.org](https://www.rcdso.org/)) provides detailed information about dentists registered in Ontario, Canada. This project involves extracting data from the website for various purposes, such as research or building databases. The scripts utilize Selenium for web scraping and automation, with support for structured data storage in CSV format.

---

## Overview of Scripts

### 1. `links.py`
#### Purpose:
This script extracts links to individual dentist profiles based on city names entered by the user.

#### Functionality:
- Navigates to a specified page URL.
- Extracts profile links based on predefined XPath locators.
- Saves the extracted links into a CSV file categorized by city names.

#### Workflow:
1. **User Input**:
   - City name (used for categorizing the CSV file).
   - Page URL (initial page to start scraping).
2. **Navigation**:
   - Opens the specified page in a browser.
   - Extracts links to profiles using XPath.
3. **Data Storage**:
   - Stores the extracted links in `C:/imp codes/Canada/citylink/<city_name>.csv`.

#### Key Methods:
- `land_first_page(url)`:
  Opens the given URL and maximizes the browser window.
- `get_links(category)`:
  Finds profile links on the current page and appends them to a CSV file categorized by city name.

#### Example Usage:
```bash
Enter City: Toronto
Enter Page Url: https://example.com/profiles
```

---

### 2. `main.py`
#### Purpose:
This script extracts detailed information from individual dentist profiles and stores it in a structured CSV format.

#### Functionality:
- Reads profile links from CSV files generated by `links.py`.
- Visits each profile link and extracts relevant details.
- Geocodes addresses to retrieve latitude and longitude using Geopy.
- Saves all extracted data to categorized CSV files.

#### Workflow:
1. **User Input**:
   - City name (used for identifying the link CSV file and categorizing data).
2. **Data Extraction**:
   - Opens each link and extracts details such as name, registration number, status, practice information, and certificates.
   - Handles missing data gracefully by assigning "N/A" for unavailable fields.
3. **Geocoding**:
   - Converts addresses into geographic coordinates using the ArcGIS geocoder.
4. **Data Storage**:
   - Saves the structured data into `path to save/<city_name>.csv`.

#### Key Methods:
- `land_required_page(url)`:
  Navigates to the specified dentist profile URL.
- `get_name()`, `get_full_name()`, `get_Registration_number()`, etc.:
  Extracts individual fields like name, registration number, and current status using predefined XPath locators.
- `get_primary_practice_information()`:
  Retrieves the primary practice's address, phone number, and geocoded coordinates.
- `get_academic_information()`:
  Extracts educational details such as dental degree and year of completion.
- `get_certificates()`:
  Retrieves details about registration certificates and their issuance dates.
- `mov_into_file(name)`:
  Writes the extracted data into a CSV file categorized by city name.

#### Example Usage:
```bash
Enter Name: Toronto
```

---

## Technologies Used

- **Selenium**:
  - Automates web page navigation and interaction.
  - Extracts data using XPath-based locators.
- **Pandas**:
  - Structures and stores extracted data into CSV files.
- **Geopy (ArcGIS)**:
  - Converts addresses into geographic coordinates.
    
---

## Recommendations

1. **Error Handling**:
   - Consider adding robust exception handling to restart from the last processed link in case of failures.
   - Log errors for easier debugging.
2. **Headless Browsing**:
   - Utilize headless browsing for faster scraping, especially in production environments.
3. **Dynamic Locators**:
   - Ensure that XPath locators are dynamic and resilient to changes in the website's structure.

---

## Conclusion

This project provides an efficient method for extracting and structuring detailed information about registered dentists in Ontario. The `links.py` script focuses on profile discovery, while the `main.py` script extracts and organizes detailed data. Together, they offer a comprehensive solution for web scraping tasks related to the RCDSO website.

